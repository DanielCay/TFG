{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR+\"/codigo\")  # To find local version of the library\n",
    "from funcionesAux import utils, visualize, metricas, obtenerMascaras, io, training\n",
    "import funcionesAux.model as modellib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar datos de entrenamiento procesados\n",
    "training.etiquetarYguardarDatos(\"samplesTesting/procesadosZgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar datos de entrenamiento procesados\n",
    "train_data1,train_data2,train_label,test_data1,test_data2,test_label=training.cargarDatos(\"samplesTesting/procesadosZgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Input, concatenate, Activation, Subtract\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "\n",
    "#(14,14,256)->(1,1,256)\n",
    "def bloqueReduccion(in1,in2):\n",
    "    x1=Conv2D(256, (5, 5), strides=(2,2))\n",
    "    x2=BatchNormalization()\n",
    "    x3=Activation('relu')\n",
    "    x4=Conv2D(256, (5, 5))\n",
    "    x5=BatchNormalization()\n",
    "    x6=Activation('relu')\n",
    "    \n",
    "    #objeto 1\n",
    "    ob1=x1(in1)\n",
    "    ob1=x2(ob1)\n",
    "    ob1=x3(ob1)\n",
    "    ob1=x4(ob1)\n",
    "    ob1=x5(ob1)\n",
    "    ob1=x6(ob1)\n",
    "    \n",
    "    #objeto 2\n",
    "    ob2=x1(in2)\n",
    "    ob2=x2(ob2)\n",
    "    ob2=x3(ob2)\n",
    "    ob2=x4(ob2)\n",
    "    ob2=x5(ob2)\n",
    "    ob2=x6(ob2)\n",
    "    \n",
    "    return ob1,ob2\n",
    "\n",
    "def createModel():\n",
    "\n",
    "    in1 = Input(shape=(14,14,256))\n",
    "    in2 = Input(shape=(14,14,256))\n",
    "\n",
    "    #(1,1,256)->(256)\n",
    "    reduccion_in1,reduccion_in2 = bloqueReduccion(in1,in2)\n",
    "    reduccion_in1 = Flatten()(reduccion_in1)\n",
    "    reduccion_in2 = Flatten()(reduccion_in2)\n",
    "\n",
    "    #(reduccion_in1-reduccion_in2)\n",
    "    z = Subtract()([reduccion_in1, reduccion_in2])\n",
    "\n",
    "    z=Dense(256, activation='relu')(z)\n",
    "    z=Dense(128, activation='relu')(z)\n",
    "    z=Dense(64, activation='relu')(z)\n",
    "    z=Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    modelRAM = Model(inputs=[in1,in2], outputs=[z])\n",
    "\n",
    "    modelRAM.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return modelRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINTAR GRAFICAS para ver como se entrena la red en cada epoca\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"redes/1/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True)\n",
    "\n",
    "# Create a new model instance\n",
    "modelRAM=createModel()\n",
    "\n",
    "modelRAM.summary() \n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "modelRAM.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "modelRAM.fit([train_data1,train_data2], train_label, callbacks=[tensorboard_callback,cp_callback], batch_size=10, validation_split=0.2, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobar la accuracy de la red para cada epoca y la epoca para la cual la accuracy es mayor\n",
    "model = createModel()\n",
    "\n",
    "test=[]\n",
    "numFichero=0\n",
    "maxAcc=0\n",
    "maxIndex=0\n",
    "for i in range(201):\n",
    "    # Load the previously saved weights\n",
    "    model.load_weights(\"redes/1/cp-\"+str(numFichero).zfill(4)+\".ckpt\")\n",
    "\n",
    "    loss,acc = model.evaluate([test_data1,test_data2], test_label, verbose=0)\n",
    "    #print(\"test loss, test acc:\", loss,acc)\n",
    "    test.append(acc)\n",
    "    if acc>maxAcc:\n",
    "        maxAcc=acc\n",
    "        maxIndex=i\n",
    "    \n",
    "    numFichero+=1\n",
    "\n",
    "plt.plot(test)\n",
    "print('acc->',maxAcc)\n",
    "print('epoca->',maxIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobar la accuracy para una epoca en concreto\n",
    "model = createModel()\n",
    "model.load_weights(\"redes/1/cp-0136.ckpt\")\n",
    "loss,acc = model.evaluate([test_data1,test_data2], test_label, verbose=0)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrar accuracy y loss de cada epoca\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 25363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrar accuracy y loss del reentrenamiento de Mask R-CNN\n",
    "%tensorboard --logdir cars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
